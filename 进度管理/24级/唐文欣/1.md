#   自瞄菜鸟学习
## 25  9月4日
1.工创垃圾分类强队代码，理解到代码的全能强队的区别，细节化代码，方便调试

2.理解到要有自己的学习路线，能系统化说出自己到底学习了什么，抛开AI自己能做到什么

3.RM论坛算法开源，自瞄，青工会有经验分享

4.在树莓派上面运载yolo11n模型   (指令篇)
Sudo apt update&&sudo apt upgrade -y 
- 激活当前环境
Mkdir yolo &&cd yolo  
- 创建并激活虚拟环境
Python3 -m venv - -system-site-packages venv  
- 创建虚拟环境
Source venv/bin/activate 
- 激活虚拟环境
- 建立虚拟环境的目的是避免python环境发生冲突，还有一层目的是如果自己的虚拟环境发生错误时对外界环境也不会产生影响，可以直接删除重新建立
关了窗口重新激活
Cd yolo  source venv/bin/activate
Pip install ultralytics ncnn
- 安装运行yolo所需要的ultralytics和·ncnn库
- 装上摄像头 
Ls /dev/video* 
- 查看当前连接的video设备，看到video0或者video1即可
Yolo detect predict model=yolo11n.pt  
- 下载运行coco的预训练模型)
Yolo export model=yolo11n.pt  format=ncnn(将模型默认的pytorch格式改为更容易在树莓派上面运行的ncnn格式
wegt https://ejtech.io/code/yolo_detect.py##下载别人的视觉检测代码
python yolo_detect.py –model=yolo11n_ncnn_model –source=usb0 --resolution=1280x720
- 运行检测代码，这个代码只是能看到模型识别的效果，实际运行逻辑代码还是要自己去写
## 9月7日
1.	colcon build 的命令根本目的是给ros2指令构造空间，当遇到报错时可以尝试删掉已经构建的build文件，重新构建 rm -rf ~/my-1/build(移除指令)
2.	赋权限指令  sudo chmod 666 /dev/tty*
3.	蓝灯亮只是电源线正确连接，蓝灯一闪一闪才代表相机正确启动
4.	在小电脑上面启动MVS图像化界面，方法一:点击左下角显示应用程序，然后再找到MVS图标    方法二：cd /opt/MVS/bin    ./MVS.sh
5.	一定要记得看报错信息，这样才能更好的排查问题，自己强大才是真的强大
## 9月8日
1.相机标定代码的运行要进行终端的拆分colcon build
source install/setup.bash
ros2 launch hik_camera hik_camera.launch.py
- exposure_time
- gain

标定程序启动(拆分一个单独的终端)
colcon build
source install/setup.bash
ros2 run camera_calibration cameracalibrator --size 8x6 --square 0.024 image:=/image_raw  camera:=/hik_camera --no-service-check
2.自瞄代码在运行的时候插两个串口，这样才能正确识别到海康相机
## 9月9日
1.	有一个奇怪问题，把海康相机插到我自己电脑的虚拟机上面，再插上虚拟串口，用lsusb命令和ls /dev/tty* 命令别都能找到两个串口和ttyUSB0，但是在小电脑上面运行lsusb命令没有问题，但是用ls /dev/tty*命令只能在最开始能检测到ttyUSB0，后面再次执行命令就找不到了，对此的解释是因为在虚拟机上面可以找到就证明串口占据和防火墙问题可以排除，从根本分析虚拟机上面由系统直接控制设备，而小电脑上面需要经过原生内核，Udev，电源管理模块等，还有可能是串口的问题
## 9月10日
1，成功运行自瞄代码，把虚拟下位机改为true,就可以不用转串口了，但是曝光度有点问题，明天调一下，也有可能是装甲板亮度不均匀，明天试试
## 9月11日
1.	发现问题了，就是甲板亮度不均匀，今天检测成功，如果执行命令出现报错，可以尝试重新colcon build 
## 9月12日
1.	不同的文件会有不同的功能
2.	接下来任务就是看垃圾桶代码，还有看无人机自瞄代码的开源，努力尝试写一个自己的无人机代码，飞镖也需要学习

## 9月13日
1.学习怎么使用github上传代码
2.看中科大无人机自瞄开源        遇到问题：看不太懂，但是又无法实践
## 9月14---9月15
1.看视觉救命群里面的视觉文档发现一些可以利用的规则：
- 如果步兵采用变加速自转，就算对方的反小陀螺算法可以捕捉到装甲板，也无法稳定击打
- 如果直接用热成像对机器人继续检测，因为rm战车会有小电脑运算放热，和弹丸发射枪管发热，这样是不是比视觉雷达更快更简单，只用识别图像中发热的红色区域
- 对抗样本生成，用图片来扰乱，目标检测算法
## 9月·16-----9月20
1.	看robomaster社区开源，自瞄，飞镖，无人机，很遗憾我没有找到飞镖的视觉开源代码
2.	运行传承飞镖代码，我明白了问·ai也要有自己的思考，高效直接质问，或者可以提出一些自己的想法子，基本步骤 mkdir build          cd build
Cmake ..     ls命令查找makefile    然后再运行make命令   之后ls命令可以找到生成的可执行文件              （遇到问题找不到flact版本为24.3.25的下载源）
3．学习同济大学视觉开源，先啃代码，demo约等于ui界面，MPC
4.做垃圾桶识别代码
5.联盟赛英雄自瞄思路： 09-20 10:38:17
- 如果是在联盟赛的话，我现在有一个很好的想法，因为英雄的射速和那个步兵的射速是不一样的。
- 英雄的话我只用保证在他的中心开火，每个装甲板只要有一颗能够打中就行了，我不需要跟随我的就视频上我的云台是不需要怎么动的，还有它的弹丸足够大，你的射出去打造装备连板的时候如果能够捕捉到他那个模型，然后就能够逐步你和那个弹丸的预测函数进行拟合，这样它的精准度就能够高很多，或者那个弹丸里面有能够定位的东西，那的定位，它的精准度就会再高一个阶层。
- 所以我需要写一个英雄自瞄代码，它能够实现每个装甲板都能够打一颗，然后还能通过模型拟合去识别弹丸的位置去逐步减小误差。

